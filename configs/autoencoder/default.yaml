module:
  target: models.vae.autoencoder.ComplexAutoencoderKL
  params:
    dim: 2
    lr: 2e-5
    weight_decay: 0.
    log_interval: 100

    enc_dec_config:
      in_channels: 3
      out_channels: 3
      in_resolution: 64
      embed_dim: 64
      hidden_dims: [128, 256, 512, 512]
      latent_dim: 4
      num_blocks: 2
      patch_size: 4
      window_size: 7
      mlp_ratio: 4
      qkv_bias: True
      num_heads: -1
      num_head_channels: 32
      drop_path: 0.1
      dropout: 0.0
      attn_dropout: 0.0
      proj_bias: True
      num_groups: 32
      act: 'silu'
      use_pos_enc: True
      dtype: 'complex64'

    loss_config:
      target: modules.loss.contperceptual.LPIPSWithDiscriminator
      params:
        disc_start: 100001
        logvar_init: 0.0
        kl_weight: 1e-6
        pixelloss_weight: 1.0
        disc_num_layers: 3
        disc_in_channels: 3
        disc_factor: 1.0
        disc_weight: 1.0
        perceptual_weight: 1.0
        use_actnorm: False
        disc_conditional: False
        disc_loss: "hinge"

        lpips_config:
          ckpt_path: 'checkpoints/lpips/swin_v2_t/lpips.ckpt'
          net_type: 'swin_v2_t'

data:
  target: modules.datasets.util.DataModuleFromConfig
  params:
    num_workers: 2
    batch_size: 2
    wrap: True
    train:
      target: modules.datasets.genshin_impact.GenshinImpactDataset
      params:
        root: '/local_datasets/genshin_impact'
        train: True
        transform_configs:
          transforms:
            - target: torchvision.transforms.transforms.ToTensor
            - target: torchvision.transforms.transforms.Resize
              params:
                size: [ 256, 256 ]
    validation:
      target: modules.datasets.genshin_impact.GenshinImpactDataset
      params:
        root: '/local_datasets/genshin_impact'
        train: False
        transform_configs:
          transforms:
            - target: torchvision.transforms.transforms.ToTensor
            - target: torchvision.transforms.transforms.Resize
              params:
                size: [ 256, 256 ]


logger:
  target: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
  params:
    save_dir: './logs'
    name: 'autoencoder'
    version: 'default'


checkpoints:
  latest_checkpoint:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      dirpath: './checkpoints/autoencoder/default'
      filename: 'autoencoder'
      monitor: 'step'
      mode: 'max'
      save_top_k: 1

  best_checkpoint:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      dirpath: './checkpoints/autoencoder/default'
      filename: 'autoencoder_best'
      monitor: 'val/rec_loss'
      mode: 'min'
      save_top_k: 1

trainer:
  accelerator: 'gpu'
  max_epochs: 5